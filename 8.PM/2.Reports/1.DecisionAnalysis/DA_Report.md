# 扒谱小组技术报告  
**基于时频分析的多乐器转录研究**  
**Automatic Music Transcription**  

## Revision List  

| 时间 | 修订内容 |  
|:---|:---|  
|02/26/2019|建档，确定三个技术方向并确定负责人|  
|03/03/2019|MIDI文件部分新增MIDI文件格式解析、文件分析器、MIDI文件转五线谱方法|  
||数据集部分新增傅里叶变换、小波变换相关内容|  
|03/09/2019|深度学习部分新增CNN简介、CNN结构、CNN实现环境相关内容|  
|03/10/2019|MIDI文件部分新增JS生成MIDI文件参考文献|  
|03/18/2019|MIDI文件部分新增格式详解（有待精简）|  
|03/19/2019|数据集部分新增librosa库内容|  
|07/31/2019|新增技术方向：Source Separation，包括相关研究与非负矩阵分解实现方法简介|  
||新增技术方向：Onset Detect|    

  

## 技术方向  

### 深度学习  
> 负责人：唐宝钰  

#### CNN简介
​	卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积计算且具有深度结构的前反馈神经网络，是深度学习的代表算法之一。卷积神经网络由具有可更新的权重和偏置常量的神经元组成。每个神经元都接收一些输入，并做一些点积计算，输出输入项对应每个分类的分数。与普通神经网络相比，卷积神经网络默认输入图像，可以让我们把特定的性质编入网络结构，使我们的前馈函数更加有效率，并减少了大量参数。

#### CNN结构  
​	CNN通常包括输入层，卷积层，池化层和全连接层：

1. **输入层**：卷积神经网络的输入层可以处理多维数据，常见地，一维卷积神经网络的输入层接收一维或二维数组，通常为时间或频谱采样；二维数组可能包含多个通道；二维卷积神经网络的输入层接收二维或三维数组；三维卷积神经网络的输入层接收四维数组 。由于卷积神经网络在计算机视觉领域有广泛应用，因此许多研究在介绍其结构时预先假设了三维输入数据，即平面上的二维像素点和RGB通道。

2. **卷积层**：卷积神经网络中每层卷积层由若干卷积单元组成，每个卷积单元的参数都是通过反向传播算法优化得到的。卷积运算的目的是提取输入的不同特征，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。

3. **池化层**：卷积层进行特征提取后，输出的特征图会被传递至池化层进行特征选择和信息过滤。池化层包含预设定的池化函数，其功能是将特征图中单个点的结果替换为其相邻区域的特征图统计量。能够得到新的、维度较小的特征。
4. **全连接层**：把所有局部特征结合变成全局特征，用来计算最后每一类的得分。

#### CNN实现环境  
选择Tensorflow作为该项目中深度学习部分应用的框架。  

**Tensorflow简介**  
	Tensorflow是广泛使用的实现机器学习以及其它涉及大量数学运算的算法库之Tensorflow由Google开发，是GitHub上最受欢迎的机器学习库之一。Google几乎在所有应用程序中都使用Tensorflow来实现机器学习。TensorFlow在图形分类、音频处理、推荐系统和自然语言处理等场景下都有丰富的应用。  

**Tensorflow特点**  
	1. 高度的灵活性  
	2. 真正的可移植性  
	3. 多语言支持  
	4. 丰富的算法库  
	5. 完善的文档  

**Tensorflow环境的搭建**  
	见参考文献\[5\]\[6\]。  

  




### MIDI文件  
> 负责人：宋云飞  

#### MIDI文件格式解析  

**简介**  
	MIDI是数字音乐的国际标准，定义了计算机音乐程序、合成器及其他电子设备交换信息和电子信号的方式，解决不同电子乐器之间不兼容的问题。  
	MIDI文件中包含音符，定时和多个通道的演奏定义。文件包括每个通道的演音符信息：键通道号、音长、音量和力度等。  

**格式说明**  
	MIDI文件中的数据划分为若干个块，块是由：块标记，块长度和块数据组成的。其中块标记为24个字节的ascii码，块长度为4个字节的数值，表示块数据域的长度。  

**MIDI文件格式解析模块的结构与解析流程**  
	MIDI文件格式解析模块需要完成的任务主要有：理解MIDI信息、分析MIDI信息、生成五线谱信息，因此可以将MIDI文件解析分为两大块。
	第一块为文件分析器，主要负责按顺序阅读MIDI文件，将各数据成分分离并分别存储，完成底层面向文件的数据读入和错误处理，保证向上层完整的正确的排列有序的音符信息。  
	第二块为整合分析器，主要负责有选择地使用文件分析器从MIDI文件中获得的原始信息，按照一系列的规则，找出音符之间的组合关系，生成五线谱的逻辑内容。  

**文件分析器**  
	头块中表明了MIDI文件类型、包含的轨道数和四分音符的时间增量数。头块的解析相对比较简单，但其中的文件类型和四分音符的时间增量数对解析流程的影响很大。  
	MIDI文件类型不同，MIDI文件的内部复杂度会有较大的差别。不过MIDI文件的三种类型之间是复杂度递增的关系，而且前一种类型可以近似地看作是后一种类型的特例，所以需要面对最复杂的类型做好处理，另外两种类型就会容易一些。  
	MIDI文件中一般把四分音符的时间增量设置为120。  

**MIDI事件的读取**  
	MIDI消息的种类非常丰富，而且没有统一的格式，这意味着只能把每一种消息单独处理。不是所有的MIDI消息都与五线谱有关，所以要真正理解的MIDI消息并不是非常多。要面对的最多的消息是音符打开消息（Note-on），它的实际使用量达到了所有消息的总是用量的95%。音符关闭消息也是被需要处理的，如果不处理这种消息，会造成音符一直不停的情况。  
	由于MIDI中没有音符的概念，因此要通过将对应的音符开启和关闭事件配对形成一个音符，称之为原始音符，之后还需要将音符开始时间戳和结束时间戳转换为音符开始时间戳和音符持续长度。  
	为了完成上述两个任务，使用一个大数组缓存16个通道里的128个音的状态。在接收到音符打开与关闭消息时进行记录，并同时计算开始时间与持续时间。  

**Meta事件的读取**
	MIDI中的Meta事件中描述的信息在五线谱显示中基本上都是有用的，有些信息还起着至关重要的作用。例如很多说明性的文字信息，需要直接添加到五线谱和各音轨的属性中。当这些说明性信息重复出现时，可以把两段信息的文字连起来，作为一条长的信息出现。  
	调号和拍号信息是Meta信息中非常重要的两条。调号决定了每个音符在五线谱上显示的确切位置及其升降号标志，拍号决定了小节的长度还同时影响合成音符组的规则。这些信息都是整合分析器不可缺少的重要信息。  

**整合分析器**  
    文件分析器只能够读出MIDI文件中直接说明的音符，如果直接显示这样的音符，得到的五线谱将非常难看。整合分析器的任务就是接收原始音符表与其它如拍号调号等信息进行排列重组，生成一个正确美观的五线谱。由于要分析的信息非常多，仅列举了一些相对重要的工作。  

**确定谱号**  
	Meta信息里只提供了调号和拍号的信息，而同样重要的谱号信息却没有提供。由于在整合分析器中的许多工作都要使用谱号信息，所以对音符的统计工作必须在进入整合分析器之前就进行完毕，即把对音符的统计穿插到文件分析器中。  
	确定谱号需要统计音轨中最高和最低音符的音高。得到了这两个值后，便可以决定使用低音谱号和高音谱号中的一种。如果最高音和最低音相差太多，以至于使用无论使用低音谱号还是使用高音谱号都不能满足要求，可以考虑将一个音轨拆成两条五线显示。  

**添加休止符**  
	休止符是五线谱中与音符同样重要的符号元素，而在原始音符表中只有实在的音符而没有休止符。休止符是指在一定的时间范围内音轨上没有任何音处于打开状态，所以只要监视文件分析器中的音状态矩阵就可以判断是非存在休止符。  

**小节划分与音符的拆分组合**  
	五线谱中的小节划分对显示来说具有两个作用，方便上下同步声部乐谱对齐和在换行时保持末端对齐。由于原始音符可能完全属于某一个小节，又或跨越若干个小节，因此就要把音符拆分成多个小的音符，并用延音符号连接他们，或者把一个小节中的几个音符组合在一起构成和弦或者共尾音符组等情况。  
	完成这一部分工作需要将原始音符表转换为有序栈。有序栈的特点保证了虽然每个音符都有可能被反复插入，但是它插入的次数与整体栈规模无关，只与音符的复杂度有关。在音符复杂度一定的情况下，每个栈元素被反复操作的次数接近一个常量，故整体函数仍停留在nlog(n)级别，是可以接受的。  
	为了降低运算量，可以利用只含有一个和弦的共尾音符组和只含有一个音符的和弦统一各种符号间的比较，把3x3=9步比较合并为两步，降低了比较的代价，大大减少了代码量。另一个优化之处是利用了有序栈，把时间复杂度降低到nlog(n)。  
      
**将MIDI文件转换为可以阅读的五线谱的方法**  
	由于这个部分主要用来进行机器学习时的正确率对照，所以直接使用了已经开发完成的软件，经过测试后可以给出准确的五线谱，我们可以使用它。最最初始的学习的时候用的简单音高可以来人工读出来。  
	这个部分我们需要进行几个步骤，首先是获得钢琴这种使用五线谱的乐器的midi文件，这样在对照的时候更加方便，在机器学习的时候对照更快。接下来在考虑多乐器的时候需要把五线谱转化为其他的谱子，比如六线谱。  

#### JS生成MIDI文件  
​	见参考文献\[7\]。  

  

  


### 数据集   
> 负责人：李燕杰  

#### 傅里叶变换、小波变换、常数Q变换  

​	在将wav文件的音频波形信号转换为时频图（spectrogram）的过程中，需要用到常数Q变换做时域与频域的分析。本部分概述快速傅里叶变换（FFT）、短时傅里叶变换（STFT）、小波变换（WT）以及常数Q变换（CQT）在音乐信号时频分析中的特点。
​	本部分参考资料详见引用\[1\]\[2\]\[3\]\[4\]\[6\]\[8\]\[9\]。  

**傅里叶变换**  
	傅立叶变换可将时域信号转换至频域信号，对一整段信号分析其频率成分。但由于分解出的三角函数为作用域为整个时域，其对时间的分辨率不高。即假设一段由一段高频信号与一段低频信号前后连接而成，那么对该信号做傅立叶变换的结果，与对二者顺序调换后形成的一段信号做傅立叶变换的结果完全相同。  

**短时傅里叶变换**  
	短时傅里叶变换通过对时域信号加窗分析，即对时域切片，逐次对每一段时域信号进行FFT。STFT较之简单FFT增加了对时间的分辨率，但仍有其局限性，即窗口大小的选择。窗口过大时将丧失时间上的分辨率（向简单FFT靠拢），窗口过小时将丧失频率上的分辨率，即不能在某一频率范围内更准确地定位到某个频率。  

**小波变换**    
	傅里叶变换的局限性主要原因在其分解出的每一个三角函数都作用在整个时域上，小波变换则修正了这一点。小波变换中使用有限长且会衰减的小波基替换了三角函数基（“小波”名称的由来），每个小波成分仅在一小段时域上作用，因此对时间有更高的解析度。  
	不同于傅里叶变换，小波变换变量只有频率ω，小波变换有两个变量：尺度a（scale）和平移量τ（translation）。尺度a控制小波函数的伸缩，平移量 τ控制小波函数的平移。尺度就对应于频率（反比），平移量 τ就对应于时间。  
	对一个时间窗口中的波形信号进行不同scale的拟合，得到该时间窗口中对应的不同频率的系数向量；移动该时间窗口（对应平移量translation），得到不同时间窗口中波形信号的频率系数向量。  
	因此对wav进行小波分析的结果输出为一个系数矩阵，对应行列分别为时间维度与频率维度，可对应绘制光谱图。  

**常数Q变换**  
	常数Q变换较之小波变换，在频率纵轴上取了对数标度，反映到音高上则为线性标度，更加接近于人耳对于音乐的认知。



#### 使用librosa库绘制视频光谱图  
参考librosa官方文档\[10\]。  

**使用`librosa.core.cqt()`对wav信号进行常数Q变换**  
	默认参数下，该函数将对传入信号进行C1到C7七个八度的分解。  

**一些转换函数**  
`librosa.core`中包含一系列`note_to_midi`、`amplitude_to_db`、`hz_to_midi`等转换函数，可在输出文件时利用。  

**使用`librosa.display.specshow()`绘制时频图**  
	默认参数与`librosa.core.cqt()`相同。  
	对其`data`参数，若直接使用CQT中输出的系数矩阵（amplitude）则会对泛音有较好的屏蔽作用，可用以识别基频音高。  
	若使用`librosa.core.amplitude_to_db(data)`对原振幅矩阵做转换，则会将泛音也展示出来，或许可在后续研究中用以乐器分析。  
	下图为《小星星变奏曲》起始七个音符的两种情况对比图。  
[C4 C4 G4 G4 A4 A4 G4](Img/Spec_contrast.png)  



### Source Separation  
多乐器的转录需要将不同乐器的音频分离，由此需进行Source Separation。采用的方法为非负矩阵分解（Non-Negative Matrix Factorization, NMF）。    
本部分参考资料详见引用\[11\]\[12\]\[13\]。  

#### 相关研究：ICA，ISA，NMF与NTF  

1. Independent Component Analysis(ICA)  
ICA（独立成分分析）目的在于将混合信号分立为尽可能统计独立的不同分量（as statistically independent as possible）。思想类似于解多元一次方程式，对同一个混合声，需要多麦克风输入才能解得独立的各个分量。因此不能直接用于单输入的盲源分离。  

2. Independent Subspace Analysis(ISA)  
ISA（独立子空间分析）与ICA相类似，但其使用短时傅里叶变换（STFT）将原始信号分离为不同频率区间。不同的频率区间可看作同一混合源的不同观察输入，因此可再应用ICA对源进行分离。  

3. Non-Negative Matrix Factorization(NMF)  
基于NMF（非负矩阵分解）的盲源分离常将模型简化为声音信号的线性叠加。矩阵表示为：${ X=BG }$，其中$X$为混合源信息，$B$为基源信息，$G$为时变增益（time-varying gain）。NMF的任务就是在只有矩阵${X}$的情况下获取$B,G$。  
其中，$X$往往为$m*t$的矩阵，也即每一行为频率点/段，每一列为时间帧。因此相应地，$B$为$m*n$矩阵，$G$为$n*t$矩阵，其中$n$可代表分离源个数。而在我们的研究中，$m$一般取88。  

4. Non-Negative Tensor Factorization(NTF)  
NTF（非负张量分解）则与NMF非常相似，但在音源信息数据表示上新增了一个维度。NMF往往使用振幅谱与时变增益，而NTF则在此基础上使用调制域与频变触发作为第三维度。  
NTF接收一个张量$X$作为输入，$X$为一个三维数组，维度分别代表频率、时间与调制。NTF的分解目标可表达为：$X_{r,n,m} = {\Sigma}^K_{k=1}G_{r,k} {\cdot} A_{n,k} {\cdot} S_{m,k}$，其中$G$，$A$，$S$分别表示增益（gain）、频率基（frequency basis）与激活（activation）。 
> While NMF decomposes a magnitude or power spectrum basis over time varying gain, NTF uses modulation domain as the third dimension and includeds frequency varying activation.   

#### Source Separation实现方法  
项目实现参考论文\[11\]，使用了librosa与scikit-learn等第三方Python库。  
1. 获取振幅矩阵$X$  
   - 通过`librosa.core.cqt()`获取频谱，但此时获取的为复数系数矩阵  
   - 通过`librosa.core.magphase()`将复数系数矩阵分解为实数振幅矩阵`S`与复数相位`P`  
   - 振幅矩阵`S`即为原始混合矩阵$X$  
2. 对$X$进行非负矩阵分解  
   - 直接使用scikit-learn中封装的NMF函数  
   - 参考\[11\]中的算法完成  
     - 随机初始化$B,G$  
     - $B$迭代：${B {\lArr} B.{\times} {\frac{{\frac{X}{BG}}G^T}{1G^T}}}$  
     - $G$迭代：${G {\lArr} G.{\times} {\frac{{\nabla}{c^-}(B,G)}{{\nabla}{c^+}(B,G)}}}$，其中：${{\nabla}{c^+}(B,G)} = {B^T}1$，${{\nabla}{c^-}(B,G)} = {B^T{\frac{X}{BG}}}$
     - Cost function: ${c(B,G)=c_r(B,G)+{\alpha}c_t(G)+{\beta}c_s(G}$  
        - $c_r(B,G)$: Reconstruction error term, using:  
          ${D(X||BG)={\Sigma}_{k,t}[X]_{k,t}{\log}{\frac{[X]_{k,t}}{[BG]_{k,t}}}-[X]_{k,t} + [BG]_{k,t}}$  
		- $c_t(G)$: Temporal continuity term, using:  
		  ${c_t(G)={\Sigma}_{j=1}^{J}{\frac{1}{{\sigma}_j^2}}{\Sigma}_{t=2}^T(g_{t,j}-g_(t-1,j)^2}, {\sigma}_j={\sqrt{(1/T){\Sigma}_{t=1}^{T}g_{t,j}^2}}$  
		- $c_s(G)$: Sparseness Objective term, using:  
		  $c_s(G) = {\Sigma}_{j=1}^J{\Sigma}_{t=1}^Tf(g_{j,t}/{\sigma}_j)$  
3. 再合成  
   这一步仅仅作为验证Source Separation效果的工具，在AMT项目集成中实际上不需要，可只将振幅频谱向下一模块传递。  
   

  

## 参考文献 

[1] [傅里叶分析教程](https://zhuanlan.zhihu.com/p/19763358)   
[2] [形象易懂讲解算法I——小波变换](https://zhuanlan.zhihu.com/p/22450818)  
[3] [小波变换完美通俗讲解系列之 （一）](https://zhuanlan.zhihu.com/p/44215123)  
[4] [小波变换完美通俗讲解系列之 （二）](https://zhuanlan.zhihu.com/p/44217268)  
[5] [WIN10安装TENSORFLOW（GPU版本）](https://zhuanlan.zhihu.com/p/37086409)  
[6] [Windows环境下安装TensorFlow并在Jupyter notebook上使用](https://blog.csdn.net/index20001/article/details/73555182)  
[7] [用JS生成MIDI文件](https://blog.csdn.net/u012767526/article/details/51510421)  
[8] [The Engineer's Ultimate Guide to Wavelet Analysis - The Wavelet Tutorial](https://cseweb.ucsd.edu/~baden/Doc/wavelets/polikar_wavelets.pdf)  
[9] [The Constant Q Transform](http://doc.ml.tu-berlin.de/bbci/material/publications/Bla_constQ.pdf)  
[10] [Librosa home](https://librosa.github.io/librosa/index.html#)  
[11] [Monaural Sound Source Separation by Nonnegative Matrix Factorization With Temporal Continuity and Sparseness Criteria](http://www.cs.tut.fi/sgn/arg/music/tuomasv/virtanen_taslp2007.pdf)  
[12] [Librosa Docs >> Advanced Examples >> Vocal Separation](https://librosa.github.io/librosa/auto_examples/plot_vocal_separation.html?highlight=vocal)  
[13] [Musical Instrument Source Separation In Unison And Monaural Mixtures](http://www.cs.bilkent.edu.tr/tech-reports/2014/BU-CE-1403.pdf)  
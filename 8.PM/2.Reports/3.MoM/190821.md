# 暑期总结  

## 项目整体进度  
1. 基本完成[暑期安排](./190625.md)  
   附原计划表。 

|时间|目标| 
|:-:|:-|   
|7.14|模块内部逻辑厘清|  
||编写接口文档（固定）|  
||**具体要求：**|  
||1. 确定各个模块的输入与输出|  
||2. 只向外暴露**1个**确定的函数接口|  
||3. 如涉及跨模块文件，确定文件格式|  
|7.23|完成 `Onset/Offset` 与 `Pitch` 模块集成|  
||进行MIDI File Generation代码实现|  
|7.31|确定 `Source Separation` 实现思路|  
||继续 `MIDI File Generation` 代码实现|  
||完成 `Pitch Detection` 内部逻辑|  
|8.10|完成 `Pitch` 与 `MIDI` 模块集成|  
||大致完成 `Source Seperation` 模块|  

2. 下一步：  
   1. 搭建项目平台  
   2. 模块按提高准确度与效率迭代  


## 模块成果  

### Source Separation  
Souce Separation所有实现几乎全部涉及到音频的频谱信息，参考论文中往往使用短时傅里叶变换（STFT）获取频谱信息，但在本项目中为确保前后信息可重用，一律使用常数Q变换（CQT）所得频谱。关于STFT与CQT间效果比较并未进行详细实验。  
1. 尝试非负矩阵分解（Non-negative Matrix Factorization, NMF）  
   1. 使用`scikit`库中默认的NMF模型  
   2. 按照所学习论文搭建NMF模型  
  **效果**：经实验，在音源混合较为简单的情况下二者表现类似；稍微复杂的情况`scikit`相对更好，但同样都无法为`pitch detection`提供足够用以识别的频谱信息。  
2. 尝试全连接神经网络（Fully Connected Network, FCN）  
   参考论文：[Monoaural Audio Source Separation Using Deep Convolutional Neural Networks](https://www.semanticscholar.org/paper/Monoaural-Audio-Source-Separation-Using-Deep-Neural-Chandna-Miron/fedef8eedef76692d805a6a3380159a95b79b4de)  
   参考论文源码：[GitHub - SConsul/audio-source-separation](https://github.com/SConsul/audio-source-separation)  
   参考论文使用PyTorch搭建神经网络，复现使用Keras。论文中使用RWC instrument sound数据集，复现使用此前为`onset detection`制作的数据集。  
   **效果**：相比NMF更能提取出完整的频谱信息供后续模块使用。但局限性较强。例如当下训练使用的为小提琴与钢琴的混合音源，因此该网络目前只对小提琴与钢琴有较好的分离效果。  

**迭代方向**：完善FCN模型，添加更多层或尝试Hourglass等网络模型；提供更大、更有代表性的训练集。  


### Onset Detection  
该部分为假期前完成实现，假期进行了与`pitch detection`模块的集成。  
1. 优化集成逻辑  
   `onset detection`要求频谱矩阵前有一段零矩阵，才能保证较好的效果；而`pitch detection`则在有零矩阵时不能保证很好的音符帧切割准确性，因此需要在程序流中加入`zero_padding`、`zero_tailing`等操作。  

2. 优化算法参数  
   起始点检测算法中包括光滑系数、以及最后的判定阈值等，不同的音乐片段应当使用不同的值，但现在仍未找到合适的判定算法。  

**迭代方向**：优化模块参数确定算法，或许可以考虑再次搭建训练网络。  


### Pitch Detection  
1. 完成了向前与`onset_detection`模块、向后与`file_generation`的集成。  
2. 模块优化  
    1. 模块包装  
       根据API文档确定模块包装思路，确定了频谱图裁剪、利用卷积神经网络循环检测音高和为score中的每个Note赋予音高的方法。完成了pitch_detect模块的初步包装。  
    2. 完成Demo神经网络训练 
       训练能够识别15个单音和54个双音的的卷积神经网络模型。  
       **效果**：准确率十分可喜。  
   **在训练过程中，发现当前的卷积神经网络为了能够识别多个音高，所需的数据集过于庞大，故针对精简数据集的问题进行了其他尝试。**  
    3. 使用单音高训练数据集识别多音高样本  
       该方法的基本思路参考人群密度算法，先通过一个卷积神经网络判定该图片含有几个音高，再根据卷积神经网络的分类结果，取音高判定概率最大的前几个结果。  
       **效果**：然而，由于训练图片的特征过于明显，使分类得到的结果过于绝对。当输入一张图片时，CNN会十分肯定的给出 属于 或者 不属于的结果，即大多数只存在[0, 0, 0, 1, 0, 0]的概率输出，而很少存在[0.1, 0.2, 0.3, 0.4, 0.1, 0.1]这样的概率结果。  
    4. 尝试使用较少的双音组合训练  
       即使用N个单音 和包含N个单音两两配对的多音数据集训练。结果并不能令人满意。  
    5. 为减少音高检测所需数据集，开始进行Object Detection有关尝试。尝试通过检测图片中横条的位置来判定音高  
       检测出一张训练图片上的每一条横条，依据各横条位置的数学关系进行音高判断  
       **结果**：存在漏框错框等现象，且较为严重。  
    6. 尝试使用OpenCV直接对频谱图中的信息进行定位  
       对图片进行归一化，二值化，模糊，膨胀，腐蚀等操作，增加一定限制条件寻找图片上的横条区域并框选。  
       **效果**：通过一系列的图片归一化和参数优化操作，基本能够识别出每一横条的位置，但是对于亮度差别不大且相邻的横条识别为一条。若想通过此方法框选最亮横条的位置，仍需图片的进一步归一化操作。  

**迭代方向**：完善归一化操作，继续尝试OpenCV。  


### File Generation
**概要**：
1.完成了File Generation的代码实现，并且与Pitch模块集成。

**完成情况**：
1.基本完成了文件生成的任务：利用已有的melo项目，机器生成其所需的melo文件来生成MIDI乐谱和mid文件，并且使用现有的score生成了完成度较高的mid文件和MIDI乐谱，与pitch模块成功集成。
**结果**：生成了乐曲star的mid文件，音准准，节拍基本正确。

2.目前的乐谱生成考虑了以下特性：
     2.1.每个乐谱有其特定的节拍，乐谱的节拍确定需要遵守节拍的规则。
     2.2.乐谱支持全音符，二分音符，四分音符，八分音符，十六分音符，三连音，延长音。
     2.3.乐谱支持多乐器的乐谱。
**结果**：对于节拍简单一些的乐谱，根据相关的音符信息，可以以较高的准确度生成相应的乐谱。

3.目前生成乐谱存在的问题：
     3.1.针对一些节拍复杂的片段，还没有很好地节拍表示的方法。
     3.2.尚未完全找到melo项目的音符表示方法，由于缺少使用说明，还需要在多次的实验中发现规律。

**迭代方向**：完善不同节拍的表达方式，提高稳定性。
